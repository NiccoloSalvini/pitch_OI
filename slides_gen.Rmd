---
title: "Text Mining in R"
subtitle: "⚔<br/>with Xaringan"
author: "Salvini Niccolò"
date: "2021/01/04 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

background-image: url(img/oi-logo.png)
background-size: contain

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE,
                      fig.height=9,
                      fig.width=16)



library(tidyverse, quietly = T, warn.conflicts = F)
library(tidytuesdayR, quietly = T, warn.conflicts = F)
library(scales, quietly = T, warn.conflicts = F)
library(tidytext, quietly = T, warn.conflicts = F)
library(lubridate, quietly = T, warn.conflicts = F)
library(tidylo, quietly = T, warn.conflicts = F)
library(magick, quietly = T, warn.conflicts = F)
library(tidyr, quietly = T, warn.conflicts = F)
library(janeaustenr, quietly = T, warn.conflicts = F)


## xaringan theme
library(xaringanthemer, quietly = T, warn.conflicts = F)
library(showtext, quietly = T, warn.conflicts = F)
# theme_set(theme_light())
# 
# custom _Officine Italia_ theme

#Define gppr_theme() function
# 
# theme_oi = function(){ 
#     font = "Georgia"   #assign font family up front
#     
#     theme_minimal() %+replace%    #replace elements we want to change
#     
#     theme(
#       
#       #grid elements
#       panel.grid.major = element_blank(),    #strip major gridlines
#       panel.grid.minor = element_blank(),    #strip minor gridlines
#       axis.ticks = element_blank(),          #strip axis ticks
#       
#       #since theme_minimal() already strips axis lines, 
#       #we don't need to do that again
#       
#       #text elements
#       plot.title = element_text(             #title
#                    family = font,            #set font family
#                    size = 20,                #set font size
#                    face = 'bold',            #bold typeface
#                    hjust = 0,                #left align
#                    vjust = 2),               #raise slightly
#       
#       plot.subtitle = element_text(          #subtitle
#                    family = font,            #font family
#                    size = 14),               #font size
#       
#       plot.caption = element_text(           #caption
#                    family = font,            #font family
#                    size = 9,                 #font size
#                    hjust = 1),               #right align
#       
#       axis.title = element_text(             #axis titles
#                    family = font,            #font family
#                    size = 10),               #font size
#       
#       axis.text = element_text(              #axis text
#                    family = font,            #axis famuly
#                    size = 9),                #font size
#       
#       axis.text.x = element_text(            #margin for axis text
#                     margin=margin(5, b = 10))
#       
#       #since the legend often requires manual tweaking 
#       #based on plot content, don't define it here
#     )
# }
# logo = image_read("img/oi-logo.png")
# grid::grid.raster(logo, x = 0.09, y = 0, just = c('left', 'bottom'), width = unit(2, 'inches'))


```

```{r xaringan-themer, include=FALSE, warning=FALSE}

style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Rubik"),
  text_font_google   = google_font("Rubik", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```


???

Image credit: [Officine Italia logo](https://images.squarespace-cdn.com/content/5e89b9646da6496e27deb3d5/1586882637764-QZHOK9GWFWMJ4TXC1KKZ/oi-logo-test-2-02.png?content-type=image%2Fpng)

---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%

# Disclaimers

- Il tema delle slides (colore e font), come i loghi delle immagini possono essere cambiati tramite semplice aggiunta di *CSS*, per ora sono autogenerati e solamente adattati nel colore e nella tipologia dei font tramite il pacchetto `xaringanthemer` 

--

- Il tema dei grafici (messi giù con `ggplot2`) può essere cambiato e adattato alla stessa maniera, in questo caso è adattato per coerenza di stile allo stesso tema autogenerato per le slides vd. sopra

--

- I seguenti grafici e pezzi di codice sono basati sulla documentazione orginale della libreria `Tidytext` e il progetto Open Source [tidytuesday](https://www.tidytuesday.com/) a cura di [David Robinson](https://github.com/dgrtwo/data-screencasts). Inoltre è utilizzata una stack di librerie legate agli autori dell'analisi e all'universo `Tidyverse`

--

- I datasets su cui sono fatti correre i codici sono resi pubblici ai seguenti indirizzi [Repo 1](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/beyonce_lyrics.csv), [Repo 2](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/taylor_swift_lyrics.csv), libri di Jane Austen e download di tweets di Gualtieri (open .csv file tramite [vicinitas](https://www.vicinitas.io/))

---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%

# Presenter Notes (keyboard shortcuts)

.pull-left[
- `h`: Help

- `c`: Copy to new window

- `p`: **Presenter mode** (premilo se devi presentare)

- `m`: Mirror

- `f`: Full screen

- `b`: Black out

- `t`: Start/stop timer

- `num + enter`: Nav to page `num`
]

---
class: inverse, center, middle

# Sentiment Analysis

```{r loaddata, echo=FALSE, message=FALSE}
beyonce_lyrics = readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/beyonce_lyrics.csv')
taylor_swift_lyrics = readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/taylor_swift_lyrics.csv')
sales = readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/sales.csv')
charts = readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/charts.csv')


tidy_books = austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

```

---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%
# Jane Austen Books

--

.left-column[
###  Research Question:
]


--

.right-column[
Quali sono le parole associate a sentimenti "**buoni**" (=joy equivalent) o "**cattivi**" nei Romanzi di [Jane Austen](https://it.wikipedia.org/wiki/Jane_Austen)?

- Sense and Sensibility 
- pride adn prejudice 
- Mansfield park
- Emma 
- narthanger abbey
- Persuasion

]

---
## Workflow Sentiment Analysis
Il testo per ogni libro subisce una serie di passaggi (scatole nere) secondo cui:

--

- Vengono **tokenizzati**, cioè ogni parola viene divisa dalla successiva

--

- Viene preso un **dizionario di sentimento** (ex. NRC) che raccoglie tutte le parole e i relativi sentimenti associati alla parola

--

- Vengono riarrangati con un **ordine** (grandezza di conteggio per esempio)

--

- Vengono **Visualizzati**


![](https://www.tidytextmining.com/images/tmwr_0201.png)

---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%
class: center, middledw
##  Visualizzati con Tabelle Interattive...

Sentiment è un punteggio che assegna uno score al sentimento (joinando ogni parola ad un suo score tramite il dizionario "Bing lexicon") ed è la differenza tra sentimento positivo e sentimento negativo.

```{r sentbuoni, echo=FALSE, message=FALSE}
library(textdata, quietly = T, warn.conflicts = F)
library(DT, quietly = T, warn.conflicts = F)
nrc_joy = get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
jane_austen_sentiment %>% 
  head(50) %>%
  DT::datatable(fillContainer = FALSE, options = list(pageLength = 5))
  # knitr::kable(format = "html")
```

---
### Visualizzati con Grafici Statici...
Puoi immaginarti questo su **diversi documenti** per più paesi comparativamente. Index tiene traccia della parte del libro (sezione) dove si è (80 linee di testo per segmento).

```{r jasent, echo=FALSE, message=FALSE}
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")+
  theme_xaringan(background_color = "#FFFFFF")
```
---
```{r jasent2, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
library(plotly, quietly = T, warn.conflicts = F)
p = ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")+
  ggtitle(label = "O Grafici Dinamici")+
  theme_xaringan(background_color = "#FFFFFF")
ggplotly(p) %>%
  hide_legend()
```


---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%
class: center, middledw
### Conteggio parole più comuni associate a sentimenti *negativi* e *positivi*, cross-libri

```{r bencounts, echo=FALSE, message=FALSE}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Le contribuzioni dei sentimenti",
       y = NULL) +
  theme_xaringan(background_color = "#FFFFFF")

```


---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%
### Rappresenta una Nuvola della parole più comuni
```{r wordcloud, echo=FALSE, message=FALSE}
library(wordcloud, quietly = T, warn.conflicts = F)
library(reshape2, quietly = T, warn.conflicts = F)

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

---
class: inverse, center, middle

# tf-idf Term Frequency (tf) - Inverse Document Frequency (idf),


---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%
###  Due parole su [tf-idf](https://it.wikipedia.org/wiki/Tf-idf)...

"The statistic *tf-idf* is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.
The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents, in this case, the group of Jane Austen’s novels as a whole."

```{r ex, echo=FALSE, message=FALSE}
book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

book_tf_idf <- book_words %>%
  bind_tf_idf(word, book, n)

book_tf_idf %>% 
  head(30) %>% 
DT::datatable(fillContainer = FALSE, options = list(pageLength = 4))
```

---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%
## td-idf score per parola-docs
```{r tfidf, echo=FALSE, message=FALSE}
book_tf_idf <- book_words %>%
  bind_tf_idf(word, book, n)

book_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf score", y = NULL) +
  theme_xaringan(background_color = "#FFFFFF")
  
```
---
class: inverse, center, middle

# Relazioni tra Parole e correlazioni, "n-grams"

---
###  Qualcosa su [n-gram](https://it.wikipedia.org/wiki/N-gramma)

"Many interesting text analyses are based on the relationships between words, whether examining which words tend to follow others immediately, or that tend to co-occur within the same documents [...]
This includes tokenizing (punto A, slide 7 ) with token = "ngrams" argument, which tokenizes by pairs of adjacent words rather than by individual ones" 
In questo caso **Bigrams** (si può fare anche **Trigrams** (dove $n = 3$ numero di parole):

```{r bigrams, echo=F, message=F}
austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>% 
  head(30) %>% 
DT::datatable(fillContainer = FALSE, options = list(pageLength = 5))
  
```
---
## Combini il bigram con il tf-idf
Consideri il bigramma come se fosse una parola sola e la tratti con tf-idf. Facendo questo eviti le parole che per sintassi e grammatica stanno sempre insieme come verbi fraseologici etc.)
```{r nbigram, echo=F, message=F}
bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(tf_idf, bigram, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ book, ncol = 2, scales = "free") +
  labs(x = "tf-idf di un bigramma", y = NULL) +
  theme_xaringan(background_color = "#FFFFFF")
```
---
### ... E riapplichi i concetti della **Sentiment**

In questo specifico caso vai a esplorare una disambiguazione comune associata alla parola "not" in inglese. Se avessimo fatto una sentiment e avessimo trovato la parola "happy" nel documento, allora il sentiment associato al documento sarebbe stato positivo. Tuttavia quando la parola è preceduta da **not** in realtà comunica l'esatto opposto. Quindi esamino le parole più comuni sia accostate ad un sentimento e anche precedute dalla parola "not":
```{r afinn, echo=F, message=F}
AFINN <- get_sentiments("afinn")

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)
not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(n * value, word2, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Valore del sentimento * # di volte che compare",
       y = "Parole precedute da \"not\"") +
  theme_xaringan(background_color = "#FFFFFF")

```

---
## Visualizzare **Network** di bigrammi

Possiamo anche essere interessati a visualizzare le relazioni tra parole simultaneamente, anzichè qualcuna in cima ad una lista: 
```{r net, echo=F, message=F, warning=F}
library(igraph, quietly = F)
library(ggraph, quietly = F)
bigram_graph <- bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame()

## setta seed
set.seed(2021)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```

---
class: inverse, center, middle

# Topic Modeling

---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%

## Due parole sul Topic Modeling

"In text mining, we often have collections of documents, such as blog posts or news articles, that we’d like to divide into **natural groups** so that we can understand them separately. Topic modeling is a method for **unsupervised classification** of such documents, **similar to clustering** on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for."

--

Ci sono diversi modelli che tirano fuori i Topics da stringhe di testo, quello più popolare è **LDA**. La teoria qui diventa un po' più pesante e la assumo.

--

I Dati in questo caso vengono da "AssociatedPress" e sono una collezione di 2246 articoli di un' agenzia americana. Il modello associa ad ogni parola uno score che indica la probabilità che quella stessa parola provenga un tema $T$, dati un numero di temi decisi dall'utente. Qui presumo $T = 2$ . 

```{r loaddare, echo=F, message=F, warning=F}
library(topicmodels, quietly = T)

data("AssociatedPress")
## fitting model
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_topics <- tidy(ap_lda, matrix = "beta")
```

### I Risultati...

---
background-image: url(img/oi-logo.png)
background-position: 0% 100%
background-size: 20%

### Topic 1 e 2

Vedo le top 10 parole e posso desumere che il tema a SX riguarda **l'Economia-Finanza**, a DX **Politica**

```{r risu, echo=F, warning=F, message=F}
ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()+ 
  theme_xaringan(background_color = "#FFFFFF")
```
---
## ... Ma puoi anche fare cosi'

Puoi anche andare a vedere le differenze maggiori tra score di probabilità, chiamala $\beta$, tra le parole e andarle a comparare nuovamente i due topics. In questa maniera filtri sole le parole meno comuni, quindi quelle potenzialmente più pregne di significato.

```{r oddsratio, message=F, warning=F, echo=F}
beta_spread <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 odds ratio dei betas tra topic 2 / topic 1", y = NULL)+ 
  theme_xaringan(background_color = "#FFFFFF")
```








<!-- Soggetti: [Julia Silge](https://twitter.com/juliasilge), [David Robinson](https://twitter.com/drob) -->

<!-- Vale la pena vedere i tweets nel tempo: -->

<!-- ```{r importtweets, echo=F, message=F, warning=F} -->
<!-- library(lubridate, quietly = T) -->
<!-- library(readr, quietly = T) -->
<!-- library(readxl, quietly = T) -->

<!-- tweets_dave <- read_csv("https://raw.githubusercontent.com/dgrtwo/tidy-text-mining/master/data/tweets_dave.csv") -->
<!-- tweets_julia <- read_csv("https://raw.githubusercontent.com/dgrtwo/tidy-text-mining/master/data/tweets_julia.csv") -->
<!-- ## due soggetti nuovi  -->
<!-- tweets_gualtieri = read_excel("data/gualtierieurope.xlsx") -->
<!-- tweets_lagarde <- read_csv("data/Lagarde.xlsx") -->


<!-- tweets <- bind_rows(tweets_julia %>%  -->
<!--                       mutate(person = "Julia"), -->
<!--                     tweets_dave %>%  -->
<!--                       mutate(person = "David")) %>% -->
<!--   mutate(timestamp = ymd_hms(timestamp)) -->
<!-- ggplot(tweets, aes(x = timestamp, fill = person)) + -->
<!--   geom_histogram(position = "identity", bins = 20, show.legend = FALSE) + -->
<!--   facet_wrap(~person, ncol = 1) -->
<!-- ``` -->




